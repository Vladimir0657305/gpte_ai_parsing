from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from urllib.parse import urljoin
import time
import pandas as pd

# Создаем список, в котором будем хранить данные
data = []

driver = webdriver.Chrome()
driver.get("https://gpte.ai/")

# Прокручиваем страницу до конца, чтобы загрузить все элементы
while True:
    # Прокручиваем страницу до конца
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")

    # Ждем, чтобы страница загрузилась
    time.sleep(15)

    # Проверяем, достигли ли мы конца страницы
    end_of_page = driver.execute_script("return window.innerHeight+window.pageYOffset >= document.body.offsetHeight;")
    if end_of_page:
        break

    # Находим ссылку на Older Posts и кликаем на нее
    older_posts_link = driver.find_element_by_css_selector('nav.pagination a.older-posts')
    older_posts_url = urljoin(driver.current_url, older_posts_link.get_attribute('href'))
    print("Processing URL:", older_posts_url)
    driver.get(older_posts_url)

    # Ждем, чтобы страница загрузилась
    time.sleep(20)

    # Находим все элементы статей на странице
    articles = driver.find_elements_by_css_selector('div.post')

    # Извлекаем информацию из каждой статьи и добавляем ее в список данных
    for article in articles:
        title = article.find_element_by_css_selector('h2.title').text
        category = article.find_element_by_css_selector('a.category').text
        description = article.find_element_by_css_selector('div.excerpt').text
        image_url = article.find_element_by_css_selector('img').get_attribute('src')

        data.append({
            'Title': title,
            'Category': category,
            'Description': description,
            'Image URL': image_url
        })

# Создаем DataFrame из списка данных
df = pd.DataFrame(data)

# Сохраняем DataFrame в CSV-файл
df.to_csv('data.csv', index=False)
